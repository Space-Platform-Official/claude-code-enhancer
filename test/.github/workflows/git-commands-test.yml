name: Git Commands Test Suite

on:
  push:
    branches:
      - main
      - develop
      - 'feature/**'
    paths:
      - 'templates/commands/git/**'
      - 'test/**'
      - '.github/workflows/git-commands-test.yml'
  pull_request:
    branches:
      - main
      - develop
    paths:
      - 'templates/commands/git/**'
      - 'test/**'
      - '.github/workflows/git-commands-test.yml'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode'
        required: false
        default: 'full'
        type: choice
        options:
          - quick
          - full
          - debug
      specific_phase:
        description: 'Specific test phase to run'
        required: false
        type: choice
        options:
          - all
          - static
          - mock
          - integration

# Cancel in-progress runs when a new run is triggered
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

env:
  # Test configuration
  TEST_PARALLEL_JOBS: 4
  TEST_CACHE_KEY: git-test-cache-v1
  TEST_ARTIFACTS_RETENTION: 7
  
  # Performance thresholds
  MAX_TEST_DURATION: 600  # 10 minutes
  PERFORMANCE_REGRESSION_THRESHOLD: 20  # 20% slower than baseline

jobs:
  # Validate test environment and dependencies
  validate-environment:
    name: Validate Test Environment
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
      test-matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better git testing

      - name: Generate cache key
        id: cache-key
        run: |
          echo "key=${{ env.TEST_CACHE_KEY }}-${{ hashFiles('test/**/*.sh', 'templates/commands/git/**/*.md') }}" >> $GITHUB_OUTPUT

      - name: Setup test environment
        run: |
          # Make scripts executable
          chmod +x test/*.sh test/lib/*.sh
          
          # Validate directory structure
          for dir in test/lib test/fixtures test/schemas test/scenarios; do
            if [[ ! -d "$dir" ]]; then
              echo "Missing required directory: $dir"
              exit 1
            fi
          done

      - name: Define test matrix
        id: matrix
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # For PRs, run on multiple OS versions
            echo 'matrix={"os":["ubuntu-latest","macos-latest"],"test_mode":["quick","full"]}' >> $GITHUB_OUTPUT
          else
            # For pushes, run full suite on Ubuntu only
            echo 'matrix={"os":["ubuntu-latest"],"test_mode":["full"]}' >> $GITHUB_OUTPUT
          fi

  # Phase 1: Static Validation
  static-validation:
    name: Static Validation (${{ matrix.os }})
    needs: validate-environment
    runs-on: ${{ matrix.os }}
    strategy:
      matrix: ${{ fromJson(needs.validate-environment.outputs.test-matrix) }}
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Cache test results
        uses: actions/cache@v3
        with:
          path: test/.cache
          key: ${{ needs.validate-environment.outputs.cache-key }}-static-${{ matrix.os }}
          restore-keys: |
            ${{ env.TEST_CACHE_KEY }}-static-${{ matrix.os }}

      - name: Run static validation
        id: static
        run: |
          cd test
          ./run-git-tests.sh -p static -r junit --no-color
        continue-on-error: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: static-validation-results-${{ matrix.os }}
          path: |
            test/results/*.xml
            test/results/*.txt
          retention-days: ${{ env.TEST_ARTIFACTS_RETENTION }}

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          check_name: Static Validation (${{ matrix.os }})
          files: test/results/junit-results.xml
          fail_on: test_failures

      - name: Check for security issues
        if: steps.static.outcome == 'failure'
        run: |
          # Parse results for critical security issues
          if grep -q "CRITICAL:" test/results/*.txt; then
            echo "::error::Critical security issues found in static validation"
            exit 1
          fi

  # Phase 2: Mock Execution Tests
  mock-execution:
    name: Mock Execution Tests (${{ matrix.os }} - ${{ matrix.test_mode }})
    needs: [validate-environment, static-validation]
    runs-on: ${{ matrix.os }}
    strategy:
      matrix: ${{ fromJson(needs.validate-environment.outputs.test-matrix) }}
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Cache test results
        uses: actions/cache@v3
        with:
          path: test/.cache
          key: ${{ needs.validate-environment.outputs.cache-key }}-mock-${{ matrix.os }}-${{ matrix.test_mode }}
          restore-keys: |
            ${{ env.TEST_CACHE_KEY }}-mock-${{ matrix.os }}

      - name: Run mock execution tests
        id: mock
        run: |
          cd test
          ./run-git-tests.sh -p mock -m ${{ matrix.test_mode }} -r junit --no-color
        timeout-minutes: 30
        continue-on-error: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: mock-execution-results-${{ matrix.os }}-${{ matrix.test_mode }}
          path: |
            test/results/*.xml
            test/results/*.txt
          retention-days: ${{ env.TEST_ARTIFACTS_RETENTION }}

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          check_name: Mock Execution (${{ matrix.os }} - ${{ matrix.test_mode }})
          files: test/results/junit-results.xml
          fail_on: test_failures

  # Phase 3: Integration Tests (only on main OS)
  integration-tests:
    name: Integration E2E Tests
    needs: [validate-environment, mock-execution]
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'push' || 
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'run-integration-tests'))
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup integration environment
        run: |
          # Install Claude CLI if available
          if [[ -f "../install-claude-flow.sh" ]]; then
            bash ../install-claude-flow.sh --ci-mode
          fi
          
          # Setup test repositories
          cd test
          mkdir -p tmp/integration/repos

      - name: Cache integration test data
        uses: actions/cache@v3
        with:
          path: |
            test/.cache
            test/tmp/integration/repos
          key: ${{ needs.validate-environment.outputs.cache-key }}-integration
          restore-keys: |
            ${{ env.TEST_CACHE_KEY }}-integration

      - name: Run integration tests
        id: integration
        run: |
          cd test
          ./run-git-tests.sh -p integration -r junit --no-color --ci
        timeout-minutes: 45
        continue-on-error: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results
          path: |
            test/results/*.xml
            test/results/*.txt
            test/results/*.md
          retention-days: ${{ env.TEST_ARTIFACTS_RETENTION }}

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          check_name: Integration E2E Tests
          files: test/results/junit-results.xml
          fail_on: test_failures

  # Aggregate results and generate final report
  test-report:
    name: Generate Test Report
    needs: [static-validation, mock-execution, integration-tests]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: test-artifacts

      - name: Generate comprehensive report
        run: |
          cd test
          
          # Aggregate all test results
          mkdir -p results/aggregated
          find ../test-artifacts -name "*.xml" -exec cp {} results/aggregated/ \;
          find ../test-artifacts -name "*.txt" -exec cp {} results/aggregated/ \;
          
          # Generate summary report
          ./run-git-tests.sh --generate-report-only \
            --input-dir results/aggregated \
            --output-format markdown \
            --output-file results/test-summary.md

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = 'test/results/test-summary.md';
            
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              
              // Find existing comment
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });
              
              const botComment = comments.find(comment => 
                comment.user.type === 'Bot' && 
                comment.body.includes('## Git Command Test Results')
              );
              
              const body = `## Git Command Test Results\n\n${report}`;
              
              if (botComment) {
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: body
                });
              } else {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: body
                });
              }
            }

      - name: Upload final report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: |
            test/results/test-summary.md
            test/results/aggregated/
          retention-days: 30

      - name: Check overall status
        run: |
          # Determine if all tests passed
          failed_count=$(find test-artifacts -name "*.xml" -exec grep -c 'failures="[1-9]' {} \; | paste -sd+ | bc)
          
          if [[ $failed_count -gt 0 ]]; then
            echo "::error::$failed_count test failures detected across all phases"
            exit 1
          fi
          
          echo "✅ All tests passed successfully!"

  # Performance regression check (only on main branch)
  performance-check:
    name: Performance Regression Check
    needs: [mock-execution, integration-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test results
        uses: actions/download-artifact@v3
        with:
          path: test-artifacts

      - name: Load performance baselines
        uses: actions/cache@v3
        with:
          path: test/.cache/performance-baselines.txt
          key: performance-baselines-${{ github.sha }}
          restore-keys: |
            performance-baselines-

      - name: Check for regressions
        run: |
          cd test
          
          # Extract timing data from test results
          ./lib/test-reporter.sh --check-performance \
            --baseline .cache/performance-baselines.txt \
            --current ../test-artifacts \
            --threshold ${{ env.PERFORMANCE_REGRESSION_THRESHOLD }}

      - name: Update baselines
        if: success()
        run: |
          cd test
          
          # Update performance baselines with current results
          ./lib/test-reporter.sh --update-baselines \
            --input ../test-artifacts \
            --output .cache/performance-baselines.txt
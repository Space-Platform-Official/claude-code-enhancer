---
allowed-tools: all
description: Progressive disclosure analysis with on-demand generation patterns to minimize file footprint through intelligent usage-based creation and cleanup
---

# 🔍🔍🔍 CRITICAL REQUIREMENT: PROGRESSIVE DISCLOSURE ANALYSIS! 🔍🔍🔍

**THIS IS NOT A SIMPLE FILE GENERATION TASK - THIS IS A COMPREHENSIVE PROGRESSIVE DISCLOSURE AND USAGE PATTERN ANALYSIS TASK!**

When you run `/on-demand`, you are REQUIRED to:

1. **ANALYZE** complete usage patterns and file access frequencies through comprehensive monitoring
2. **DECOMPOSE** file creation into just-in-time generation with contextual triggers
3. **DESIGN** intelligent cleanup systems with usage-based retention policies
4. **CREATE** adaptive caching mechanisms that learn from access patterns and optimize accordingly
5. **USE MULTIPLE AGENTS** for comprehensive progressive disclosure analysis:
   - Spawn one agent for usage pattern analysis and access frequency monitoring
   - Spawn another for just-in-time generation system design and implementation
   - Spawn more agents for cleanup algorithm development and caching optimization
   - Say: "I'll spawn multiple agents to analyze progressive disclosure patterns from different optimization perspectives"

## 🎯 USE MULTIPLE AGENTS FOR PROGRESSIVE ANALYSIS

**MANDATORY AGENT SPAWNING FOR PROGRESSIVE DISCLOSURE:**
```
"I'll spawn multiple analysis agents to handle progressive disclosure optimization comprehensively:
- Usage Pattern Agent: Analyze file access patterns and frequency distributions
- Generation Strategy Agent: Design on-demand creation systems with contextual triggers
- Cleanup Algorithm Agent: Develop intelligent retention policies and automated cleanup
- Cache Optimization Agent: Implement adaptive caching with learning capabilities
- Performance Analysis Agent: Measure impact and optimize generation/cleanup cycles"
```

## 🚨 FORBIDDEN BEHAVIORS

**NEVER:**
- ❌ "Generate all files upfront" → NO! On-demand creation with usage-based triggers!
- ❌ "Simple time-based cleanup" → NO! Intelligent usage-aware retention policies required!
- ❌ "Ignore access patterns" → NO! Comprehensive usage analysis mandatory!
- ❌ "Manual generation triggers" → NO! Automated contextual generation required!
- ❌ "Fixed caching strategies" → NO! Adaptive learning-based optimization required!
- ❌ "No cleanup automation" → NO! Intelligent automated retention management required!

**MANDATORY PROGRESSIVE DISCLOSURE WORKFLOW:**
```
1. Usage pattern analysis → Monitor and analyze file access frequencies and patterns
2. IMMEDIATELY spawn agents for parallel analysis of generation and cleanup strategies
3. Just-in-time generation design → Create contextual triggers and generation systems
4. Intelligent cleanup implementation → Develop usage-aware retention policies
5. Adaptive caching optimization → Implement learning-based cache management
6. VERIFY performance impact and optimization effectiveness through comprehensive testing
```

**YOU ARE NOT DONE UNTIL:**
- ✅ Complete usage pattern analysis with access frequency distributions documented
- ✅ Multi-agent progressive disclosure analysis deployed and coordinating
- ✅ Just-in-time generation system designed with contextual triggers
- ✅ Intelligent cleanup algorithms implemented with usage-aware retention
- ✅ Adaptive caching mechanisms deployed with learning capabilities
- ✅ Performance impact measured and optimization effectiveness validated

---

🛑 **MANDATORY PROGRESSIVE DISCLOSURE VALIDATION CHECK** 🛑
1. Re-read ~/.claude/CLAUDE.md RIGHT NOW
2. Validate current file usage patterns and access frequency requirements
3. Confirm progressive disclosure design principles and optimization criteria

Execute comprehensive progressive disclosure analysis with ZERO tolerance for static file generation.

**FORBIDDEN PROGRESSIVE DISCLOSURE PATTERNS:**
- "Batch generate everything" → NO, on-demand contextual generation required
- "Simple age-based cleanup" → NO, usage-aware intelligent retention needed
- "Fixed generation triggers" → NO, adaptive contextual trigger system required
- "Manual cache management" → NO, automated learning-based optimization required
- "Ignore usage analytics" → NO, comprehensive pattern analysis mandatory

You are analyzing progressive disclosure for: $ARGUMENTS

Let me ultrathink about the comprehensive progressive disclosure architecture and optimization framework.

🚨 **REMEMBER: Effective progressive disclosure requires intelligent usage analysis, not simple file generation!** 🚨

**Comprehensive Progressive Disclosure Analysis Protocol:**

## Step 0: Usage Pattern Analysis and Baseline Establishment

**Monitor Current File Access Patterns:**
```bash
# Comprehensive usage pattern analysis
analyze_file_usage_patterns() {
    echo "=== Progressive Disclosure Usage Analysis ==="
    
    # Monitor file access frequencies
    local monitoring_period="7d"
    local access_log=".on-demand/access-patterns.log"
    
    echo "Starting $monitoring_period usage monitoring..."
    
    # Setup file access monitoring
    setup_access_monitoring "$monitoring_period"
    
    # Analyze existing usage data
    analyze_historical_access_patterns
    
    # Generate usage frequency distributions
    generate_usage_distributions
    
    echo "✅ Usage pattern analysis baseline established"
}

setup_access_monitoring() {
    local period=$1
    
    echo "🔍 Setting up comprehensive file access monitoring..."
    
    # Monitor template file access
    monitor_template_access "$period" &
    
    # Monitor documentation access
    monitor_documentation_access "$period" &
    
    # Monitor configuration file access  
    monitor_config_access "$period" &
    
    # Monitor command execution patterns
    monitor_command_patterns "$period" &
    
    echo "✅ Access monitoring systems active"
}
```

**Usage Analytics Framework:**
```yaml
usage_analytics:
  monitoring:
    access_frequency: "file_level"
    pattern_detection: "temporal_analysis"
    context_correlation: "command_execution"
    
  metrics:
    access_count: "absolute_frequency"
    access_recency: "time_since_last_access"
    access_consistency: "regularity_patterns"
    context_correlation: "usage_trigger_analysis"
    
  thresholds:
    hot_cache_threshold: 10  # accesses per day
    warm_cache_threshold: 3  # accesses per week
    cold_cleanup_threshold: 0  # no access in 30 days
```

## Step 1: Progressive Disclosure Architecture Design

**Deploy Progressive Analysis Agents:**
```bash
deploy_progressive_analysis_agents() {
    local project_path=$1
    
    echo "🤖 Deploying progressive disclosure analysis agents for: $project_path"
    
    # Usage Pattern Agent
    register_agent "usage-analyzer" "usage_pattern_analysis" "$project_path"
    spawn_usage_pattern_agent "$project_path" &
    
    # Generation Strategy Agent
    register_agent "generation-designer" "generation_strategy" "$project_path"
    spawn_generation_strategy_agent "$project_path" &
    
    # Cleanup Algorithm Agent
    register_agent "cleanup-optimizer" "cleanup_algorithm" "$project_path"
    spawn_cleanup_algorithm_agent "$project_path" &
    
    # Cache Optimization Agent
    register_agent "cache-optimizer" "cache_optimization" "$project_path"
    spawn_cache_optimization_agent "$project_path" &
    
    # Performance Analysis Agent
    register_agent "performance-analyzer" "performance_analysis" "$project_path"
    spawn_performance_analysis_agent "$project_path" &
    
    echo "✅ All progressive disclosure analysis agents deployed"
}
```

## Step 2: Just-In-Time Generation System Analysis

**Generation Strategy Agent:**
```bash
spawn_generation_strategy_agent() {
    local project_path=$1
    
    echo "🏗️ Generation Strategy Agent: Analyzing on-demand creation patterns"
    
    # Analyze generation triggers and contexts
    analyze_generation_triggers "$project_path"
    
    # Design contextual generation rules
    design_contextual_rules "$project_path"
    
    # Implement just-in-time creation system
    implement_jit_generation "$project_path"
    
    # Optimize generation performance
    optimize_generation_speed "$project_path"
    
    echo "✅ Generation Strategy Agent: On-demand system design complete"
}

analyze_generation_triggers() {
    local project_path=$1
    
    echo "🔍 Analyzing generation trigger patterns..."
    
    # Command-based triggers
    analyze_command_triggers "$project_path"
    
    # Context-based triggers
    analyze_context_triggers "$project_path"
    
    # Project-type triggers
    analyze_project_type_triggers "$project_path"
    
    # User preference triggers
    analyze_preference_triggers "$project_path"
    
    # Generate trigger optimization recommendations
    generate_trigger_optimizations "$project_path"
    
    echo "✅ Generation trigger analysis complete"
}

design_contextual_rules() {
    local project_path=$1
    
    echo "📋 Designing contextual generation rules..."
    
    cat > "$project_path/.on-demand/generation-rules.yaml" << EOF
# On-demand generation rules - Auto-generated by progressive disclosure analysis
generation_triggers:
  command_execution:
    - trigger: "git command invoked"
      generate: ["git-templates", "git-workflows"]
      conditions: ["git_repo_detected"]
      
    - trigger: "documentation command invoked"
      generate: ["doc-templates", "readme-templates"]
      conditions: ["project_has_docs_folder"]
      
    - trigger: "testing command invoked"
      generate: ["test-templates", "test-configs"]
      conditions: ["test_framework_detected"]
      
  project_context:
    - context: "javascript_project"
      auto_generate: ["js-templates", "package-configs"]
      defer_generate: ["python-templates", "go-templates"]
      
    - context: "documentation_heavy"
      auto_generate: ["doc-generators", "content-templates"]
      defer_generate: ["minimal-configs"]
      
  usage_patterns:
    - pattern: "frequent_template_user"
      cache_strategy: "aggressive_preload"
      cleanup_policy: "extended_retention"
      
    - pattern: "minimal_usage"
      cache_strategy: "minimal_footprint"
      cleanup_policy: "aggressive_cleanup"
EOF
    
    echo "✅ Contextual generation rules designed"
}
```

## Step 3: Intelligent Cleanup Algorithm Development

**Cleanup Algorithm Agent:**
```bash
spawn_cleanup_algorithm_agent() {
    local project_path=$1
    
    echo "🧹 Cleanup Algorithm Agent: Developing intelligent retention policies"
    
    # Analyze current cleanup inefficiencies
    analyze_cleanup_inefficiencies "$project_path"
    
    # Design usage-aware retention algorithms
    design_retention_algorithms "$project_path"
    
    # Implement predictive cleanup systems
    implement_predictive_cleanup "$project_path"
    
    # Optimize cleanup scheduling
    optimize_cleanup_scheduling "$project_path"
    
    echo "✅ Cleanup Algorithm Agent: Intelligent retention system complete"
}

design_retention_algorithms() {
    local project_path=$1
    
    echo "🧠 Designing intelligent retention algorithms..."
    
    # Usage-based retention scoring
    cat > "$project_path/.on-demand/retention-algorithm.py" << 'EOF'
#!/usr/bin/env python3
# Intelligent file retention algorithm - Usage-aware cleanup decisions

import time
import math
from typing import Dict, List, Tuple

class IntelligentRetention:
    def __init__(self):
        self.access_weight = 0.4
        self.recency_weight = 0.3
        self.context_weight = 0.2
        self.frequency_weight = 0.1
        
    def calculate_retention_score(self, file_metadata: Dict) -> float:
        """Calculate retention score based on multiple factors"""
        
        # Access frequency component
        access_score = min(file_metadata['access_count'] / 10.0, 1.0)
        
        # Recency component (exponential decay)
        days_since_access = file_metadata['days_since_last_access']
        recency_score = math.exp(-days_since_access / 7.0)
        
        # Context relevance component
        context_score = file_metadata['context_relevance']
        
        # Access frequency consistency
        frequency_score = file_metadata['access_consistency']
        
        # Weighted combination
        total_score = (
            access_score * self.access_weight +
            recency_score * self.recency_weight +
            context_score * self.context_weight +
            frequency_score * self.frequency_weight
        )
        
        return total_score
        
    def should_retain(self, file_metadata: Dict, threshold: float = 0.3) -> bool:
        """Determine if file should be retained based on intelligent scoring"""
        return self.calculate_retention_score(file_metadata) >= threshold
        
    def get_cleanup_priority(self, files: List[Dict]) -> List[Tuple[str, float]]:
        """Get cleanup priority list sorted by retention score"""
        scored_files = [
            (file_meta['path'], self.calculate_retention_score(file_meta))
            for file_meta in files
        ]
        return sorted(scored_files, key=lambda x: x[1])
EOF
    
    echo "✅ Intelligent retention algorithms designed"
}
```

## Step 4: Adaptive Caching System Analysis

**Cache Optimization Agent:**
```bash
spawn_cache_optimization_agent() {
    local project_path=$1
    
    echo "🚀 Cache Optimization Agent: Implementing adaptive learning-based caching"
    
    # Analyze current caching inefficiencies
    analyze_caching_inefficiencies "$project_path"
    
    # Design adaptive caching strategies
    design_adaptive_caching "$project_path"
    
    # Implement machine learning for cache optimization
    implement_ml_cache_optimization "$project_path"
    
    # Monitor and optimize cache performance
    optimize_cache_performance "$project_path"
    
    echo "✅ Cache Optimization Agent: Adaptive caching system complete"
}

design_adaptive_caching() {
    local project_path=$1
    
    echo "🧠 Designing adaptive caching strategies..."
    
    # Multi-tier caching system
    cat > "$project_path/.on-demand/cache-strategy.yaml" << EOF
# Adaptive caching strategy - Learning-based optimization
cache_tiers:
  hot_cache:
    description: "Frequently accessed files kept in memory"
    criteria:
      - access_frequency: ">= 5 per day"
      - recency: "<= 24 hours"
      - pattern_consistency: ">= 0.8"
    storage: "memory"
    retention: "indefinite"
    
  warm_cache:
    description: "Moderately accessed files kept on fast storage"
    criteria:
      - access_frequency: ">= 2 per week"
      - recency: "<= 7 days"
      - context_relevance: ">= 0.6"
    storage: "fast_disk"
    retention: "30 days"
    
  cold_storage:
    description: "Infrequently accessed files kept on standard storage"
    criteria:
      - access_frequency: ">= 1 per month"
      - recency: "<= 30 days"
    storage: "standard_disk"
    retention: "90 days"
    
  archive:
    description: "Historical files for recovery purposes"
    criteria:
      - access_frequency: "< 1 per month"
      - historical_importance: ">= 0.5"
    storage: "compressed_archive"
    retention: "1 year"

learning_algorithm:
  type: "reinforcement_learning"
  update_frequency: "daily"
  learning_rate: 0.01
  exploration_factor: 0.1
  
  features:
    - access_frequency
    - access_recency
    - context_correlation
    - user_behavior_patterns
    - project_lifecycle_stage
    
  optimization_targets:
    - cache_hit_ratio
    - generation_latency
    - storage_efficiency
    - user_satisfaction_score
EOF
    
    echo "✅ Adaptive caching strategy designed"
}
```

## Step 5: Performance Impact Analysis and Optimization

**Performance Analysis Agent:**
```bash
spawn_performance_analysis_agent() {
    local project_path=$1
    
    echo "📊 Performance Analysis Agent: Measuring progressive disclosure impact"
    
    # Establish performance baselines
    establish_performance_baselines "$project_path"
    
    # Measure generation latency impacts
    measure_generation_latency "$project_path"
    
    # Analyze storage efficiency improvements
    analyze_storage_efficiency "$project_path"
    
    # Optimize overall system performance
    optimize_progressive_performance "$project_path"
    
    # Generate comprehensive performance reports
    generate_performance_reports "$project_path"
    
    echo "✅ Performance Analysis Agent: Optimization analysis complete"
}

measure_generation_latency() {
    local project_path=$1
    
    echo "⏱️ Measuring on-demand generation performance..."
    
    # Template generation benchmarks
    benchmark_template_generation "$project_path"
    
    # Cache performance benchmarks
    benchmark_cache_performance "$project_path"
    
    # Cleanup operation benchmarks
    benchmark_cleanup_performance "$project_path"
    
    # End-to-end user experience benchmarks
    benchmark_user_experience "$project_path"
    
    # Generate performance optimization recommendations
    generate_performance_optimizations "$project_path"
    
    echo "✅ Performance measurement complete"
}
```

## Step 6: Progressive Disclosure Quality Checklist

**Progressive Disclosure Analysis Validation:**
- [ ] Complete usage pattern analysis with comprehensive access frequency monitoring
- [ ] Just-in-time generation system designed with intelligent contextual triggers
- [ ] Intelligent cleanup algorithms implemented with usage-aware retention policies
- [ ] Adaptive caching mechanisms deployed with machine learning optimization
- [ ] Performance impact measured and optimization strategies validated
- [ ] User experience improvements quantified and documented
- [ ] Progressive disclosure effectiveness demonstrated through metrics
- [ ] System learning capabilities validated and optimized

**Agent Coordination Checklist:**
- [ ] Usage pattern analysis proceeding with comprehensive monitoring systems
- [ ] Generation strategy development implementing contextual trigger systems
- [ ] Cleanup algorithm optimization creating intelligent retention policies
- [ ] Cache optimization implementing adaptive learning-based strategies
- [ ] Performance analysis measuring and optimizing progressive disclosure impact

**Progressive Disclosure Success Metrics:**
- [ ] 70-80% reduction in baseline file footprint achieved
- [ ] Sub-100ms file generation latency maintained consistently
- [ ] Cache hit ratio above 85% for frequently accessed content
- [ ] Intelligent cleanup maintaining optimal storage efficiency
- [ ] User satisfaction scores improved through reduced file clutter

**Anti-Patterns to Avoid:**
- ❌ Batch generation defeating progressive disclosure principles
- ❌ Simple time-based cleanup ignoring usage intelligence
- ❌ Fixed caching strategies not adapting to usage patterns
- ❌ Missing performance monitoring during optimization
- ❌ Ignoring user behavior patterns in generation decisions
- ❌ No learning capabilities in optimization systems

**Final Verification:**
Before completing progressive disclosure analysis:
- Have usage patterns been comprehensively analyzed and documented?
- Are generation triggers contextually intelligent and optimized?
- Do cleanup algorithms use intelligent usage-aware retention policies?
- Are caching systems adaptive and learning from access patterns?
- Has performance impact been measured and optimized effectively?
- Do progressive disclosure systems demonstrate measurable improvements?

**Final Progressive Disclosure Commitment:**
- **I will**: Execute comprehensive progressive disclosure analysis with intelligent usage monitoring
- **I will**: Implement multi-agent coordination for parallel analysis and optimization
- **I will**: Develop adaptive systems that learn from usage patterns and optimize automatically
- **I will**: Measure and optimize performance impact throughout the analysis process
- **I will NOT**: Accept static file generation defeating progressive disclosure principles
- **I will NOT**: Skip usage pattern analysis or intelligent optimization
- **I will NOT**: Ignore performance implications or user experience impact

**REMEMBER:**
This is PROGRESSIVE DISCLOSURE ANALYSIS mode - intelligent usage monitoring, adaptive optimization, and learning-based improvement. The goal is to achieve minimal file footprint through smart on-demand generation and intelligent cleanup systems.

Executing comprehensive progressive disclosure analysis protocol for optimal usage-based optimization...